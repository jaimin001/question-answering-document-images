{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaiminsg/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/jaiminsg/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modeling_layoutlm import LayoutLMForTokenClassification\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    ")\n",
    "from utils_docvqa import (\n",
    "    read_docvqa_examples,\n",
    "    convert_examples_to_features)\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "from transformers.data.processors.squad import SquadResult\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"./models/\"\n",
    "SAMPLE_DATA = \"./image.json\"\n",
    "LABELS = [\"start\",\"end\"]\n",
    "pad_token_label_id=-100\n",
    "labels = [\"start\",\"end\"]\n",
    "max_seq_length = 512\n",
    "max_query_length = 64\n",
    "doc_stride = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")\n",
    "# torch.cuda.set_device(device)\n",
    "model_class = LayoutLMForTokenClassification\n",
    "config_class = BertConfig\n",
    "tokenizer_class = BertTokenizer\n",
    "config = config_class.from_pretrained(MODEL_FOLDER,num_labels=2,cache_dir=None)\n",
    "model = model_class.from_pretrained(MODEL_FOLDER)\n",
    "tokenizer = tokenizer_class.from_pretrained(MODEL_FOLDER,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = read_docvqa_examples(SAMPLE_DATA, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 1000000000\n",
      "INFO:tensorflow:example_index: 0\n",
      "INFO:tensorflow:doc_span_index: 0\n",
      "INFO:tensorflow:tokens: [CLS] hello there [SEP] ( exception to sf - 50 approved by bureau of the budget december 1965 ) gas civil service com ##iss ##ion fe ##m cha ##p . 2000 ##0 & notification of personnel action & ( employee — see general information on reverse ) ( for agency use ) 2 . ( for agency use ) 3 . birth date 4 . social security no . 1 . name ( caps ) last — first — middle mr . — miss — mrs ( mo , day , year ) 6 . tenure group 7 . service com ##p . date | 8 . physical handicap code , robert e . dr . 80 ##19 ##5 | _ 09 - 02 - 14 49 ##9 - 34 - 05 ##9 ##7 5 . veteran preference 3 - 10 pt . di ##sa ##b . 5 - 10 pt . other 4 — 10 pt . com ##p . 9 . fe ##gli 10 . retirement 2 ‘ covered 2 — ineligible 3 - waived fi ##e pa ##ela ##te - cs 3 - f ##s 5 — other | | 2 - fi ##ca 4 — none 12 . code nature of action [ 3 effective date * civil service or other legal authority 322 | separation 04 - 30 - 67 i ee 171 | ex ##c app ##t nt ##e 04 - 30 - 68 intermittent 05 - 01 - 67 42 usc 209 £ ] 17 . ( a ) grade ( b ) step 18 . salary or or level rate ss for cs ##c use ) 15 . from : position title and number 16 . pay plan and occupation code 19 . name and location of employing office a ea ##e ae 21 . pay plan and occupation code 20 . to : position title and number 22 . ( a ) ce ##e ( b ) os 23 . salary level rate consultant t | | _ e ##g - 06 ##0 ##2 pd $ 70 . 00 ban ##n arab o ##cs on ore ##mel ##o wing op ##gic ##e national institutes of health , beth ##es ##da , maryland 2001 ##4 the director international research section 25 . duty station ( city — county — state ) 26 . location code st . louis , missouri 244 ##24 ##01 ##8 ##9 27 . app ##ropriation 28 , position occupied | 29 . app ##ort ##ioned position 1 — competitive service [ from : li ##sson ##re 34 ##8 - 770 ##01 - 22 die le ##rane ##t ama ##s pc ##wai ##ved - 2 t a es 30 . remarks : | a . subject to completion of 1 year probation ##ary ( or trial ) period commencing | b . service counting toward career ( or permanent ) tenure from : [ pe separation ##s : show reasons below , as required . check if applicable : eat se bro ##ei ##on ba ##ar sh | d . [SEP]\n",
      "INFO:tensorflow:token_to_orig_map: 4:0 5:0 6:1 7:2 8:2 9:2 10:3 11:4 12:5 13:6 14:7 15:8 16:9 17:10 18:10 19:11 20:12 21:13 22:14 23:14 24:14 25:15 26:15 27:16 28:16 29:16 30:17 31:17 32:18 33:19 34:20 35:21 36:22 37:23 38:24 39:25 40:26 41:27 42:28 43:29 44:30 45:31 46:31 47:32 48:32 49:33 50:34 51:34 52:35 53:35 54:36 55:36 56:37 57:38 58:38 59:39 60:39 61:40 62:41 63:42 64:42 65:43 66:44 67:45 68:45 69:46 70:46 71:47 72:48 73:48 74:48 75:49 76:49 77:49 78:49 79:49 80:50 81:50 82:50 83:50 84:50 85:50 86:51 87:51 88:51 89:52 90:52 91:53 92:53 93:54 94:54 95:55 96:56 97:57 98:57 99:58 100:59 101:59 102:59 103:60 104:61 105:62 106:62 107:63 108:64 109:65 110:66 111:67 112:68 113:68 114:69 115:69 116:70 117:70 118:70 119:71 120:71 121:71 122:71 123:71 124:71 125:71 126:72 127:72 128:72 129:72 130:72 131:72 132:72 133:72 134:73 135:73 136:74 137:75 138:76 139:76 140:76 141:77 142:77 143:78 144:78 145:78 146:78 147:79 148:79 149:79 150:80 151:80 152:81 153:82 154:82 155:82 156:83 157:83 158:84 159:84 160:84 161:85 162:85 163:86 164:86 165:87 166:87 167:88 168:89 169:90 170:90 171:91 172:91 173:91 174:92 175:92 176:92 177:93 178:93 179:94 180:94 181:94 182:94 183:94 184:95 185:95 186:95 187:95 188:96 189:96 190:96 191:97 192:98 193:99 194:99 195:99 196:99 197:100 198:100 199:100 200:101 201:101 202:102 203:103 204:104 205:105 206:106 207:106 208:107 209:108 210:109 211:110 212:111 213:112 214:113 215:114 216:115 217:116 218:116 219:117 220:118 221:118 222:118 223:118 224:118 225:119 226:120 227:121 228:121 229:122 230:122 231:123 232:123 233:124 234:124 235:125 236:125 237:125 238:125 239:125 240:126 241:127 242:127 243:127 244:127 245:127 246:128 247:129 248:130 249:131 250:132 251:132 252:132 253:133 254:133 255:133 256:134 257:135 258:135 259:135 260:136 261:137 262:137 263:138 264:139 265:140 266:141 267:142 268:143 269:144 270:145 271:145 272:146 273:146 274:147 275:147 276:148 277:148 278:149 279:150 280:151 281:152 282:153 283:153 284:154 285:155 286:156 287:157 288:158 289:159 290:159 291:160 292:161 293:162 294:163 295:164 296:165 297:166 298:167 299:167 300:168 301:169 302:169 303:170 304:171 305:172 306:173 307:174 308:175 309:175 310:176 311:176 312:177 313:178 314:179 315:180 316:181 317:181 318:182 319:182 320:182 321:183 322:183 323:184 324:184 325:184 326:185 327:186 328:186 329:187 330:188 331:189 332:190 333:191 334:192 335:193 336:194 337:194 338:194 339:194 340:194 341:194 342:194 343:195 344:196 345:196 346:196 347:196 348:197 349:197 350:198 351:199 352:199 353:200 354:201 355:201 356:201 357:202 358:203 359:203 360:203 361:204 362:205 363:206 364:207 365:207 366:208 367:208 368:208 369:208 370:209 371:210 372:210 373:211 374:212 375:213 376:214 377:215 378:216 379:216 380:217 381:218 382:219 383:219 384:219 385:219 386:219 387:219 388:219 389:220 390:220 391:221 392:222 393:223 394:223 395:224 396:224 397:225 398:226 399:226 400:226 401:226 402:226 403:227 404:227 405:228 406:228 407:229 408:229 409:230 410:231 411:232 412:233 413:233 414:234 415:234 416:234 417:235 418:236 419:236 420:236 421:237 422:238 423:238 424:238 425:239 426:239 427:239 428:240 429:240 430:240 431:240 432:240 433:240 434:240 435:241 436:242 437:242 438:242 439:243 440:243 441:244 442:244 443:244 444:244 445:244 446:245 447:246 448:247 449:248 450:248 451:249 452:249 453:250 454:251 455:251 456:252 457:253 458:254 459:255 460:256 461:257 462:258 463:258 464:259 465:259 466:260 467:260 468:261 469:262 470:263 471:264 472:264 473:265 474:266 475:267 476:268 477:269 478:269 479:270 480:270 481:271 482:272 483:272 484:273 485:273 486:274 487:274 488:274 489:275 490:276 491:277 492:277 493:278 494:279 495:279 496:280 497:281 498:282 499:282 500:283 501:284 502:285 503:285 504:285 505:286 506:286 507:287 508:288 509:289 510:289\n",
      "INFO:tensorflow:token_is_max_context: 4:True 5:True 6:True 7:True 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False 383:False 384:False 385:False 386:False 387:False 388:False 389:False 390:False 391:False 392:False 393:False 394:False 395:False 396:False 397:False 398:False 399:False 400:False 401:False 402:False 403:False 404:False 405:False 406:False 407:False 408:False 409:False 410:False 411:False 412:False 413:False 414:False 415:False 416:False 417:False 418:False 419:False 420:False 421:False 422:False 423:False 424:False 425:False 426:False 427:False 428:False 429:False 430:False 431:False 432:False 433:False 434:False 435:False 436:False 437:False 438:False 439:False 440:False 441:False 442:False 443:False 444:False 445:False 446:False 447:False 448:False 449:False 450:False 451:False 452:False 453:False 454:False 455:False 456:False 457:False 458:False 459:False 460:False 461:False 462:False 463:False 464:False 465:False 466:False 467:False 468:False 469:False 470:False 471:False 472:False 473:False 474:False 475:False 476:False 477:False 478:False 479:False 480:False 481:False 482:False 483:False 484:False 485:False 486:False 487:False 488:False 489:False 490:False 491:False 492:False 493:False 494:False 495:False 496:False 497:False 498:False 499:False 500:False 501:False 502:False 503:False 504:False 505:False 506:False 507:False 508:False 509:False 510:False\n",
      "INFO:tensorflow:input_ids: 101 7592 2045 102 1006 6453 2000 16420 1011 2753 4844 2011 4879 1997 1996 5166 2285 3551 1007 3806 2942 2326 4012 14643 3258 10768 2213 15775 2361 1012 2456 2692 1004 26828 1997 5073 2895 1004 1006 7904 1517 2156 2236 2592 2006 7901 1007 1006 2005 4034 2224 1007 1016 1012 1006 2005 4034 2224 1007 1017 1012 4182 3058 1018 1012 2591 3036 2053 1012 1015 1012 2171 1006 9700 1007 2197 1517 2034 1517 2690 2720 1012 1517 3335 1517 3680 1006 9587 1010 2154 1010 2095 1007 1020 1012 7470 2177 1021 1012 2326 4012 2361 1012 3058 1064 1022 1012 3558 15822 3642 1010 2728 1041 1012 2852 1012 3770 16147 2629 1064 1035 5641 1011 6185 1011 2403 4749 2683 1011 4090 1011 5709 2683 2581 1019 1012 8003 12157 1017 1011 2184 13866 1012 4487 3736 2497 1012 1019 1011 2184 13866 1012 2060 1018 1517 2184 13866 1012 4012 2361 1012 1023 1012 10768 25394 2184 1012 5075 1016 1520 3139 1016 1517 22023 1017 1011 16301 10882 2063 6643 10581 2618 1011 20116 1017 1011 1042 2015 1019 1517 2060 1064 1064 1016 1011 10882 3540 1018 1517 3904 2260 1012 3642 3267 1997 2895 1031 1017 4621 3058 1008 2942 2326 2030 2060 3423 3691 23768 1064 8745 5840 1011 2382 1011 6163 1045 25212 18225 1064 4654 2278 10439 2102 23961 2063 5840 1011 2382 1011 6273 23852 5709 1011 5890 1011 6163 4413 15529 19348 1069 1033 2459 1012 1006 1037 1007 3694 1006 1038 1007 3357 2324 1012 10300 2030 2030 2504 3446 7020 2005 20116 2278 2224 1007 2321 1012 2013 1024 2597 2516 1998 2193 2385 1012 3477 2933 1998 6139 3642 2539 1012 2171 1998 3295 1997 15440 2436 1037 19413 2063 29347 2538 1012 3477 2933 1998 6139 3642 2322 1012 2000 1024 2597 2516 1998 2193 2570 1012 1006 1037 1007 8292 2063 1006 1038 1007 9808 2603 1012 10300 2504 3446 8930 1056 1064 1064 1035 1041 2290 1011 5757 2692 2475 22851 1002 3963 1012 4002 7221 2078 5424 1051 6169 2006 10848 10199 2080 3358 6728 12863 2063 2120 12769 1997 2740 1010 7014 2229 2850 1010 5374 2541 2549 1996 2472 2248 2470 2930 2423 1012 4611 2276 1006 2103 1517 2221 1517 2110 1007 2656 1012 3295 3642 2358 1012 3434 1010 5284 24194 18827 24096 2620 2683 2676 1012 10439 26121 2654 1010 2597 4548 1064 2756 1012 10439 11589 19798 2597 1015 1517 6975 2326 1031 2013 1024 5622 7092 2890 4090 2620 1011 29065 24096 1011 2570 3280 3393 18053 2102 25933 2015 7473 21547 7178 1011 1016 1056 1037 9686 2382 1012 12629 1024 1064 1037 1012 3395 2000 6503 1997 1015 2095 19703 5649 1006 2030 3979 1007 2558 25819 1064 1038 1012 2326 10320 2646 2476 1006 2030 4568 1007 7470 2013 1024 1031 21877 8745 2015 1024 2265 4436 2917 1010 2004 3223 1012 4638 2065 12711 1024 4521 7367 22953 7416 2239 8670 2906 14021 1064 1040 1012 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 1000000001\n",
      "INFO:tensorflow:example_index: 0\n",
      "INFO:tensorflow:doc_span_index: 1\n",
      "INFO:tensorflow:tokens: [CLS] hello there [SEP] ##9 ##7 5 . veteran preference 3 - 10 pt . di ##sa ##b . 5 - 10 pt . other 4 — 10 pt . com ##p . 9 . fe ##gli 10 . retirement 2 ‘ covered 2 — ineligible 3 - waived fi ##e pa ##ela ##te - cs 3 - f ##s 5 — other | | 2 - fi ##ca 4 — none 12 . code nature of action [ 3 effective date * civil service or other legal authority 322 | separation 04 - 30 - 67 i ee 171 | ex ##c app ##t nt ##e 04 - 30 - 68 intermittent 05 - 01 - 67 42 usc 209 £ ] 17 . ( a ) grade ( b ) step 18 . salary or or level rate ss for cs ##c use ) 15 . from : position title and number 16 . pay plan and occupation code 19 . name and location of employing office a ea ##e ae 21 . pay plan and occupation code 20 . to : position title and number 22 . ( a ) ce ##e ( b ) os 23 . salary level rate consultant t | | _ e ##g - 06 ##0 ##2 pd $ 70 . 00 ban ##n arab o ##cs on ore ##mel ##o wing op ##gic ##e national institutes of health , beth ##es ##da , maryland 2001 ##4 the director international research section 25 . duty station ( city — county — state ) 26 . location code st . louis , missouri 244 ##24 ##01 ##8 ##9 27 . app ##ropriation 28 , position occupied | 29 . app ##ort ##ioned position 1 — competitive service [ from : li ##sson ##re 34 ##8 - 770 ##01 - 22 die le ##rane ##t ama ##s pc ##wai ##ved - 2 t a es 30 . remarks : | a . subject to completion of 1 year probation ##ary ( or trial ) period commencing | b . service counting toward career ( or permanent ) tenure from : [ pe separation ##s : show reasons below , as required . check if applicable : eat se bro ##ei ##on ba ##ar sh | d . from appointment of 6 months or less . special government employee . intermittent employment totaled 23 days from 05 - 01 - 66 thru 02 - 23 - 67 . service under this appointment estimated to be 30 days a year . subject to conflict of interest laws as a speech . al government e f i os se ~ date of appointment af ##fi ##dav ##it ( accession ##s only ) 3 ##¢ ##f ag ##nat ##ure ( or other authentication ) and title sr ##t john m . sang ##ster . office maintaining personnel folder ( if different from employing office ) chief of personnel , ni ##h ’ 33 . code employing department or agency he - 02 ##400 department of health , education , [SEP]\n",
      "INFO:tensorflow:token_to_orig_map: 4:72 5:72 6:73 7:73 8:74 9:75 10:76 11:76 12:76 13:77 14:77 15:78 16:78 17:78 18:78 19:79 20:79 21:79 22:80 23:80 24:81 25:82 26:82 27:82 28:83 29:83 30:84 31:84 32:84 33:85 34:85 35:86 36:86 37:87 38:87 39:88 40:89 41:90 42:90 43:91 44:91 45:91 46:92 47:92 48:92 49:93 50:93 51:94 52:94 53:94 54:94 55:94 56:95 57:95 58:95 59:95 60:96 61:96 62:96 63:97 64:98 65:99 66:99 67:99 68:99 69:100 70:100 71:100 72:101 73:101 74:102 75:103 76:104 77:105 78:106 79:106 80:107 81:108 82:109 83:110 84:111 85:112 86:113 87:114 88:115 89:116 90:116 91:117 92:118 93:118 94:118 95:118 96:118 97:119 98:120 99:121 100:121 101:122 102:122 103:123 104:123 105:124 106:124 107:125 108:125 109:125 110:125 111:125 112:126 113:127 114:127 115:127 116:127 117:127 118:128 119:129 120:130 121:131 122:132 123:132 124:132 125:133 126:133 127:133 128:134 129:135 130:135 131:135 132:136 133:137 134:137 135:138 136:139 137:140 138:141 139:142 140:143 141:144 142:145 143:145 144:146 145:146 146:147 147:147 148:148 149:148 150:149 151:150 152:151 153:152 154:153 155:153 156:154 157:155 158:156 159:157 160:158 161:159 162:159 163:160 164:161 165:162 166:163 167:164 168:165 169:166 170:167 171:167 172:168 173:169 174:169 175:170 176:171 177:172 178:173 179:174 180:175 181:175 182:176 183:176 184:177 185:178 186:179 187:180 188:181 189:181 190:182 191:182 192:182 193:183 194:183 195:184 196:184 197:184 198:185 199:186 200:186 201:187 202:188 203:189 204:190 205:191 206:192 207:193 208:194 209:194 210:194 211:194 212:194 213:194 214:194 215:195 216:196 217:196 218:196 219:196 220:197 221:197 222:198 223:199 224:199 225:200 226:201 227:201 228:201 229:202 230:203 231:203 232:203 233:204 234:205 235:206 236:207 237:207 238:208 239:208 240:208 241:208 242:209 243:210 244:210 245:211 246:212 247:213 248:214 249:215 250:216 251:216 252:217 253:218 254:219 255:219 256:219 257:219 258:219 259:219 260:219 261:220 262:220 263:221 264:222 265:223 266:223 267:224 268:224 269:225 270:226 271:226 272:226 273:226 274:226 275:227 276:227 277:228 278:228 279:229 280:229 281:230 282:231 283:232 284:233 285:233 286:234 287:234 288:234 289:235 290:236 291:236 292:236 293:237 294:238 295:238 296:238 297:239 298:239 299:239 300:240 301:240 302:240 303:240 304:240 305:240 306:240 307:241 308:242 309:242 310:242 311:243 312:243 313:244 314:244 315:244 316:244 317:244 318:245 319:246 320:247 321:248 322:248 323:249 324:249 325:250 326:251 327:251 328:252 329:253 330:254 331:255 332:256 333:257 334:258 335:258 336:259 337:259 338:260 339:260 340:261 341:262 342:263 343:264 344:264 345:265 346:266 347:267 348:268 349:269 350:269 351:270 352:270 353:271 354:272 355:272 356:273 357:273 358:274 359:274 360:274 361:275 362:276 363:277 364:277 365:278 366:279 367:279 368:280 369:281 370:282 371:282 372:283 373:284 374:285 375:285 376:285 377:286 378:286 379:287 380:288 381:289 382:289 383:290 384:291 385:292 386:293 387:294 388:295 389:296 390:296 391:297 392:298 393:299 394:299 395:300 396:301 397:302 398:303 399:304 400:305 401:306 402:306 403:306 404:306 405:306 406:307 407:308 408:308 409:308 410:308 411:308 412:308 413:309 414:310 415:311 416:312 417:313 418:314 419:315 420:316 421:317 422:318 423:319 424:319 425:320 426:321 427:322 428:323 429:324 430:325 431:326 432:327 433:328 434:328 435:328 436:329 437:330 438:331 439:332 440:333 441:334 442:335 443:336 444:337 445:338 446:339 447:339 448:339 449:339 450:340 451:340 452:340 453:341 454:341 455:342 456:342 457:342 458:343 459:343 460:343 461:344 462:344 463:345 464:346 465:346 466:347 467:348 468:349 469:349 470:350 471:351 472:351 473:352 474:352 475:353 476:354 477:355 478:356 479:357 480:358 481:358 482:359 483:360 484:361 485:362 486:362 487:363 488:364 489:365 490:365 491:366 492:366 493:367 494:368 495:368 496:369 497:370 498:371 499:372 500:373 501:374 502:374 503:374 504:374 505:375 506:376 507:377 508:377 509:378 510:378\n",
      "INFO:tensorflow:token_is_max_context: 4:False 5:False 6:False 7:False 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:False 135:False 136:False 137:False 138:False 139:False 140:False 141:False 142:False 143:False 144:False 145:False 146:False 147:False 148:False 149:False 150:False 151:False 152:False 153:False 154:False 155:False 156:False 157:False 158:False 159:False 160:False 161:False 162:False 163:False 164:False 165:False 166:False 167:False 168:False 169:False 170:False 171:False 172:False 173:False 174:False 175:False 176:False 177:False 178:False 179:False 180:False 181:False 182:False 183:False 184:False 185:False 186:False 187:False 188:False 189:False 190:False 191:False 192:False 193:False 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:False 323:False 324:False 325:False 326:False 327:False 328:False 329:False 330:False 331:False 332:False 333:False 334:False 335:False 336:False 337:False 338:False 339:False 340:False 341:False 342:False 343:False 344:False 345:False 346:False 347:False 348:False 349:False 350:False 351:False 352:False 353:False 354:False 355:False 356:False 357:False 358:False 359:False 360:False 361:False 362:False 363:False 364:False 365:False 366:False 367:False 368:False 369:False 370:False 371:False 372:False 373:False 374:False 375:False 376:False 377:False 378:False 379:False 380:False 381:False 382:False 383:False 384:False 385:False 386:False 387:False 388:False 389:False 390:False 391:False 392:False 393:False 394:False 395:False 396:False 397:False 398:False 399:False 400:False 401:False 402:False 403:False 404:False 405:False 406:False 407:False 408:False 409:False 410:False 411:False 412:False 413:False 414:False 415:False 416:False 417:False 418:False 419:False 420:False 421:False 422:False 423:False 424:False 425:False 426:False 427:False 428:False 429:False 430:False 431:False 432:False 433:False 434:False 435:False 436:False 437:False 438:False 439:False 440:False 441:False 442:False 443:False 444:False 445:False 446:False 447:False 448:False 449:False 450:False 451:False 452:False 453:False 454:False 455:False 456:False 457:False 458:False 459:False 460:False 461:False 462:False 463:False 464:False 465:False 466:False 467:False 468:False 469:False 470:False 471:False 472:False 473:False 474:False 475:False 476:False 477:False 478:False 479:False 480:False 481:False 482:False 483:False 484:False 485:False 486:False 487:False 488:False 489:False 490:False 491:False 492:False 493:False 494:False 495:False 496:False 497:False 498:False 499:False 500:False 501:False 502:False 503:False 504:False 505:False 506:False 507:False 508:False 509:False 510:False\n",
      "INFO:tensorflow:input_ids: 101 7592 2045 102 2683 2581 1019 1012 8003 12157 1017 1011 2184 13866 1012 4487 3736 2497 1012 1019 1011 2184 13866 1012 2060 1018 1517 2184 13866 1012 4012 2361 1012 1023 1012 10768 25394 2184 1012 5075 1016 1520 3139 1016 1517 22023 1017 1011 16301 10882 2063 6643 10581 2618 1011 20116 1017 1011 1042 2015 1019 1517 2060 1064 1064 1016 1011 10882 3540 1018 1517 3904 2260 1012 3642 3267 1997 2895 1031 1017 4621 3058 1008 2942 2326 2030 2060 3423 3691 23768 1064 8745 5840 1011 2382 1011 6163 1045 25212 18225 1064 4654 2278 10439 2102 23961 2063 5840 1011 2382 1011 6273 23852 5709 1011 5890 1011 6163 4413 15529 19348 1069 1033 2459 1012 1006 1037 1007 3694 1006 1038 1007 3357 2324 1012 10300 2030 2030 2504 3446 7020 2005 20116 2278 2224 1007 2321 1012 2013 1024 2597 2516 1998 2193 2385 1012 3477 2933 1998 6139 3642 2539 1012 2171 1998 3295 1997 15440 2436 1037 19413 2063 29347 2538 1012 3477 2933 1998 6139 3642 2322 1012 2000 1024 2597 2516 1998 2193 2570 1012 1006 1037 1007 8292 2063 1006 1038 1007 9808 2603 1012 10300 2504 3446 8930 1056 1064 1064 1035 1041 2290 1011 5757 2692 2475 22851 1002 3963 1012 4002 7221 2078 5424 1051 6169 2006 10848 10199 2080 3358 6728 12863 2063 2120 12769 1997 2740 1010 7014 2229 2850 1010 5374 2541 2549 1996 2472 2248 2470 2930 2423 1012 4611 2276 1006 2103 1517 2221 1517 2110 1007 2656 1012 3295 3642 2358 1012 3434 1010 5284 24194 18827 24096 2620 2683 2676 1012 10439 26121 2654 1010 2597 4548 1064 2756 1012 10439 11589 19798 2597 1015 1517 6975 2326 1031 2013 1024 5622 7092 2890 4090 2620 1011 29065 24096 1011 2570 3280 3393 18053 2102 25933 2015 7473 21547 7178 1011 1016 1056 1037 9686 2382 1012 12629 1024 1064 1037 1012 3395 2000 6503 1997 1015 2095 19703 5649 1006 2030 3979 1007 2558 25819 1064 1038 1012 2326 10320 2646 2476 1006 2030 4568 1007 7470 2013 1024 1031 21877 8745 2015 1024 2265 4436 2917 1010 2004 3223 1012 4638 2065 12711 1024 4521 7367 22953 7416 2239 8670 2906 14021 1064 1040 1012 2013 6098 1997 1020 2706 2030 2625 1012 2569 2231 7904 1012 23852 6107 23596 2603 2420 2013 5709 1011 5890 1011 5764 27046 6185 1011 2603 1011 6163 1012 2326 2104 2023 6098 4358 2000 2022 2382 2420 1037 2095 1012 3395 2000 4736 1997 3037 4277 2004 1037 4613 1012 2632 2231 1041 1042 1045 9808 7367 1066 3058 1997 6098 21358 8873 29045 4183 1006 16993 2015 2069 1007 1017 29645 2546 12943 19833 5397 1006 2030 2060 27280 1007 1998 2516 5034 2102 2198 1049 1012 6369 6238 1012 2436 8498 5073 19622 1006 2065 2367 2013 15440 2436 1007 2708 1997 5073 1010 9152 2232 1521 3943 1012 3642 15440 2533 2030 4034 2002 1011 6185 29537 2533 1997 2740 1010 2495 1010 102\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 1000000002\n",
      "INFO:tensorflow:example_index: 0\n",
      "INFO:tensorflow:doc_span_index: 2\n",
      "INFO:tensorflow:tokens: [CLS] hello there [SEP] step 18 . salary or or level rate ss for cs ##c use ) 15 . from : position title and number 16 . pay plan and occupation code 19 . name and location of employing office a ea ##e ae 21 . pay plan and occupation code 20 . to : position title and number 22 . ( a ) ce ##e ( b ) os 23 . salary level rate consultant t | | _ e ##g - 06 ##0 ##2 pd $ 70 . 00 ban ##n arab o ##cs on ore ##mel ##o wing op ##gic ##e national institutes of health , beth ##es ##da , maryland 2001 ##4 the director international research section 25 . duty station ( city — county — state ) 26 . location code st . louis , missouri 244 ##24 ##01 ##8 ##9 27 . app ##ropriation 28 , position occupied | 29 . app ##ort ##ioned position 1 — competitive service [ from : li ##sson ##re 34 ##8 - 770 ##01 - 22 die le ##rane ##t ama ##s pc ##wai ##ved - 2 t a es 30 . remarks : | a . subject to completion of 1 year probation ##ary ( or trial ) period commencing | b . service counting toward career ( or permanent ) tenure from : [ pe separation ##s : show reasons below , as required . check if applicable : eat se bro ##ei ##on ba ##ar sh | d . from appointment of 6 months or less . special government employee . intermittent employment totaled 23 days from 05 - 01 - 66 thru 02 - 23 - 67 . service under this appointment estimated to be 30 days a year . subject to conflict of interest laws as a speech . al government e f i os se ~ date of appointment af ##fi ##dav ##it ( accession ##s only ) 3 ##¢ ##f ag ##nat ##ure ( or other authentication ) and title sr ##t john m . sang ##ster . office maintaining personnel folder ( if different from employing office ) chief of personnel , ni ##h ’ 33 . code employing department or agency he - 02 ##400 department of health , education , and welfare 35 . date ( 5 - 01 - 67 l ##wt public health service 1 . employee copy source : https : / aw ##w ##w . industry ##do ##cum ##ents . uc ##sf . ed [SEP]\n",
      "INFO:tensorflow:token_to_orig_map: 4:136 5:137 6:137 7:138 8:139 9:140 10:141 11:142 12:143 13:144 14:145 15:145 16:146 17:146 18:147 19:147 20:148 21:148 22:149 23:150 24:151 25:152 26:153 27:153 28:154 29:155 30:156 31:157 32:158 33:159 34:159 35:160 36:161 37:162 38:163 39:164 40:165 41:166 42:167 43:167 44:168 45:169 46:169 47:170 48:171 49:172 50:173 51:174 52:175 53:175 54:176 55:176 56:177 57:178 58:179 59:180 60:181 61:181 62:182 63:182 64:182 65:183 66:183 67:184 68:184 69:184 70:185 71:186 72:186 73:187 74:188 75:189 76:190 77:191 78:192 79:193 80:194 81:194 82:194 83:194 84:194 85:194 86:194 87:195 88:196 89:196 90:196 91:196 92:197 93:197 94:198 95:199 96:199 97:200 98:201 99:201 100:201 101:202 102:203 103:203 104:203 105:204 106:205 107:206 108:207 109:207 110:208 111:208 112:208 113:208 114:209 115:210 116:210 117:211 118:212 119:213 120:214 121:215 122:216 123:216 124:217 125:218 126:219 127:219 128:219 129:219 130:219 131:219 132:219 133:220 134:220 135:221 136:222 137:223 138:223 139:224 140:224 141:225 142:226 143:226 144:226 145:226 146:226 147:227 148:227 149:228 150:228 151:229 152:229 153:230 154:231 155:232 156:233 157:233 158:234 159:234 160:234 161:235 162:236 163:236 164:236 165:237 166:238 167:238 168:238 169:239 170:239 171:239 172:240 173:240 174:240 175:240 176:240 177:240 178:240 179:241 180:242 181:242 182:242 183:243 184:243 185:244 186:244 187:244 188:244 189:244 190:245 191:246 192:247 193:248 194:248 195:249 196:249 197:250 198:251 199:251 200:252 201:253 202:254 203:255 204:256 205:257 206:258 207:258 208:259 209:259 210:260 211:260 212:261 213:262 214:263 215:264 216:264 217:265 218:266 219:267 220:268 221:269 222:269 223:270 224:270 225:271 226:272 227:272 228:273 229:273 230:274 231:274 232:274 233:275 234:276 235:277 236:277 237:278 238:279 239:279 240:280 241:281 242:282 243:282 244:283 245:284 246:285 247:285 248:285 249:286 250:286 251:287 252:288 253:289 254:289 255:290 256:291 257:292 258:293 259:294 260:295 261:296 262:296 263:297 264:298 265:299 266:299 267:300 268:301 269:302 270:303 271:304 272:305 273:306 274:306 275:306 276:306 277:306 278:307 279:308 280:308 281:308 282:308 283:308 284:308 285:309 286:310 287:311 288:312 289:313 290:314 291:315 292:316 293:317 294:318 295:319 296:319 297:320 298:321 299:322 300:323 301:324 302:325 303:326 304:327 305:328 306:328 307:328 308:329 309:330 310:331 311:332 312:333 313:334 314:335 315:336 316:337 317:338 318:339 319:339 320:339 321:339 322:340 323:340 324:340 325:341 326:341 327:342 328:342 329:342 330:343 331:343 332:343 333:344 334:344 335:345 336:346 337:346 338:347 339:348 340:349 341:349 342:350 343:351 344:351 345:352 346:352 347:353 348:354 349:355 350:356 351:357 352:358 353:358 354:359 355:360 356:361 357:362 358:362 359:363 360:364 361:365 362:365 363:366 364:366 365:367 366:368 367:368 368:369 369:370 370:371 371:372 372:373 373:374 374:374 375:374 376:374 377:375 378:376 379:377 380:377 381:378 382:378 383:379 384:380 385:381 386:381 387:381 388:382 389:382 390:382 391:382 392:382 393:382 394:383 395:383 396:384 397:385 398:386 399:387 400:387 401:388 402:389 403:390 404:390 405:391 406:391 407:391 408:391 409:391 410:391 411:391 412:391 413:391 414:391 415:391 416:391 417:391 418:391 419:391 420:391\n",
      "INFO:tensorflow:token_is_max_context: 4:False 5:False 6:False 7:False 8:False 9:False 10:False 11:False 12:False 13:False 14:False 15:False 16:False 17:False 18:False 19:False 20:False 21:False 22:False 23:False 24:False 25:False 26:False 27:False 28:False 29:False 30:False 31:False 32:False 33:False 34:False 35:False 36:False 37:False 38:False 39:False 40:False 41:False 42:False 43:False 44:False 45:False 46:False 47:False 48:False 49:False 50:False 51:False 52:False 53:False 54:False 55:False 56:False 57:False 58:False 59:False 60:False 61:False 62:False 63:False 64:False 65:False 66:False 67:False 68:False 69:False 70:False 71:False 72:False 73:False 74:False 75:False 76:False 77:False 78:False 79:False 80:False 81:False 82:False 83:False 84:False 85:False 86:False 87:False 88:False 89:False 90:False 91:False 92:False 93:False 94:False 95:False 96:False 97:False 98:False 99:False 100:False 101:False 102:False 103:False 104:False 105:False 106:False 107:False 108:False 109:False 110:False 111:False 112:False 113:False 114:False 115:False 116:False 117:False 118:False 119:False 120:False 121:False 122:False 123:False 124:False 125:False 126:False 127:False 128:False 129:False 130:False 131:False 132:False 133:False 134:False 135:False 136:False 137:False 138:False 139:False 140:False 141:False 142:False 143:False 144:False 145:False 146:False 147:False 148:False 149:False 150:False 151:False 152:False 153:False 154:False 155:False 156:False 157:False 158:False 159:False 160:False 161:False 162:False 163:False 164:False 165:False 166:False 167:False 168:False 169:False 170:False 171:False 172:False 173:False 174:False 175:False 176:False 177:False 178:False 179:False 180:False 181:False 182:False 183:False 184:False 185:False 186:False 187:False 188:False 189:False 190:False 191:False 192:False 193:False 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True 275:True 276:True 277:True 278:True 279:True 280:True 281:True 282:True 283:True 284:True 285:True 286:True 287:True 288:True 289:True 290:True 291:True 292:True 293:True 294:True 295:True 296:True 297:True 298:True 299:True 300:True 301:True 302:True 303:True 304:True 305:True 306:True 307:True 308:True 309:True 310:True 311:True 312:True 313:True 314:True 315:True 316:True 317:True 318:True 319:True 320:True 321:True 322:True 323:True 324:True 325:True 326:True 327:True 328:True 329:True 330:True 331:True 332:True 333:True 334:True 335:True 336:True 337:True 338:True 339:True 340:True 341:True 342:True 343:True 344:True 345:True 346:True 347:True 348:True 349:True 350:True 351:True 352:True 353:True 354:True 355:True 356:True 357:True 358:True 359:True 360:True 361:True 362:True 363:True 364:True 365:True 366:True 367:True 368:True 369:True 370:True 371:True 372:True 373:True 374:True 375:True 376:True 377:True 378:True 379:True 380:True 381:True 382:True 383:True 384:True 385:True 386:True 387:True 388:True 389:True 390:True 391:True 392:True 393:True 394:True 395:True 396:True 397:True 398:True 399:True 400:True 401:True 402:True 403:True 404:True 405:True 406:True 407:True 408:True 409:True 410:True 411:True 412:True 413:True 414:True 415:True 416:True 417:True 418:True 419:True 420:True\n",
      "INFO:tensorflow:input_ids: 101 7592 2045 102 3357 2324 1012 10300 2030 2030 2504 3446 7020 2005 20116 2278 2224 1007 2321 1012 2013 1024 2597 2516 1998 2193 2385 1012 3477 2933 1998 6139 3642 2539 1012 2171 1998 3295 1997 15440 2436 1037 19413 2063 29347 2538 1012 3477 2933 1998 6139 3642 2322 1012 2000 1024 2597 2516 1998 2193 2570 1012 1006 1037 1007 8292 2063 1006 1038 1007 9808 2603 1012 10300 2504 3446 8930 1056 1064 1064 1035 1041 2290 1011 5757 2692 2475 22851 1002 3963 1012 4002 7221 2078 5424 1051 6169 2006 10848 10199 2080 3358 6728 12863 2063 2120 12769 1997 2740 1010 7014 2229 2850 1010 5374 2541 2549 1996 2472 2248 2470 2930 2423 1012 4611 2276 1006 2103 1517 2221 1517 2110 1007 2656 1012 3295 3642 2358 1012 3434 1010 5284 24194 18827 24096 2620 2683 2676 1012 10439 26121 2654 1010 2597 4548 1064 2756 1012 10439 11589 19798 2597 1015 1517 6975 2326 1031 2013 1024 5622 7092 2890 4090 2620 1011 29065 24096 1011 2570 3280 3393 18053 2102 25933 2015 7473 21547 7178 1011 1016 1056 1037 9686 2382 1012 12629 1024 1064 1037 1012 3395 2000 6503 1997 1015 2095 19703 5649 1006 2030 3979 1007 2558 25819 1064 1038 1012 2326 10320 2646 2476 1006 2030 4568 1007 7470 2013 1024 1031 21877 8745 2015 1024 2265 4436 2917 1010 2004 3223 1012 4638 2065 12711 1024 4521 7367 22953 7416 2239 8670 2906 14021 1064 1040 1012 2013 6098 1997 1020 2706 2030 2625 1012 2569 2231 7904 1012 23852 6107 23596 2603 2420 2013 5709 1011 5890 1011 5764 27046 6185 1011 2603 1011 6163 1012 2326 2104 2023 6098 4358 2000 2022 2382 2420 1037 2095 1012 3395 2000 4736 1997 3037 4277 2004 1037 4613 1012 2632 2231 1041 1042 1045 9808 7367 1066 3058 1997 6098 21358 8873 29045 4183 1006 16993 2015 2069 1007 1017 29645 2546 12943 19833 5397 1006 2030 2060 27280 1007 1998 2516 5034 2102 2198 1049 1012 6369 6238 1012 2436 8498 5073 19622 1006 2065 2367 2013 15440 2436 1007 2708 1997 5073 1010 9152 2232 1521 3943 1012 3642 15440 2533 2030 4034 2002 1011 6185 29537 2533 1997 2740 1010 2495 1010 1998 7574 3486 1012 3058 1006 1019 1011 5890 1011 6163 1048 26677 2270 2740 2326 1015 1012 7904 6100 3120 1024 16770 1024 1013 22091 2860 2860 1012 3068 3527 24894 11187 1012 15384 22747 1012 3968 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "features = convert_examples_to_features(\n",
    "            examples=examples,\n",
    "            label_list=labels,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            pad_token_label_id=pad_token_label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "all_bboxes = torch.tensor([f.boxes for f in features], dtype=torch.long)\n",
    "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "eval_dataset = TensorDataset(\n",
    "        all_input_ids, all_input_mask, all_segment_ids,all_bboxes,all_example_index)\n",
    "eval_batch_size = 1\n",
    "eval_sampler = (\n",
    "        SequentialSampler(eval_dataset))\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0, 1000,   58,   58,  114,  126,  126,  126,   59,  107,\n",
      "          121,  160,  172,  191,   58,  110,  110,   59,   83,  102,  144,  144,\n",
      "          144,   58,   58,   83,   83,   83,  141,  141,  271,  304,  394,  416,\n",
      "          493,  544,  318,  322,  366,  378,  396,  428,  477,  489,  489,   72,\n",
      "           72,   97,  139,  139,  419,  419,  429,  429,  454,  495,  495,  533,\n",
      "          533,  544,  582,  648,  648,  658,  693,  742,  742,   56,   56,   66,\n",
      "           95,   95,   95,  132,  132,  132,  132,  132,  306,  306,  306,  306,\n",
      "          306,  306,  541,  541,  541,  564,  564,  584,  584,  420,  420,  430,\n",
      "          470,  533,  533,  543,  583,  583,  583,  616,  640,  649,  649,  660,\n",
      "          707,  755,  114,  129,  187,  187,  345,  345,  429,  429,  429,  525,\n",
      "          525,  525,  525,  525,  525,  525,  661,  661,  661,  661,  661,  661,\n",
      "          661,  661,   53,   53,   67,  111,  198,  198,  198,  219,  219,  234,\n",
      "          234,  234,  234,  303,  303,  303,  324,  324,  339,  198,  198,  198,\n",
      "          220,  220,  231,  231,  231,   56,   56,   67,   67,  415,  415,  431,\n",
      "           71,  105,  105,  212,  212,  212,  328,  328,  328,  409,  409,  429,\n",
      "          429,  429,  429,  429,  519,  519,  519,  519,  585,  585,  585,   96,\n",
      "          449,  456,  456,  456,  456,  520,  520,  520,   57,   57,   70,  105,\n",
      "          144,  159,  408,  408,  430,  482,  525,  545,  572,  613,  629,  663,\n",
      "          694,   71,   71,  112,  204,  204,  204,  204,  204,  279,  409,   72,\n",
      "           72,  112,  112,  144,  144,  186,  186,  220,  220,  220,  220,  220,\n",
      "          296,  429,  429,  429,  429,  429,  545,  570,  604,  637,  525,  525,\n",
      "          525,  545,  545,  545,  559,  599,  599,  599,  614,  647,  647,  661,\n",
      "          568,  618,  562,  612,  504,  646,  684,  684,  705,  705,   64,   64,\n",
      "           78,   78,  122,  167,  196,  219,  415,  415,  429,  450,  476,  430,\n",
      "          491,   64,   64,   78,  106,  129,  178,  193,  250,  343,  345,  345,\n",
      "          404,  415,  415,  429,  450,  476,  429,  489,   63,   63,   77,   77,\n",
      "          103,  149,  178,  200,  529,  529,  544,  544,  544,  558,  558,  598,\n",
      "          598,  598,  613,  647,  647,  663,  559,  611,   70,  525,  409,  409,\n",
      "          428,  428,  428,  428,  428,  428,  428,  662,  687,  687,  687,  687,\n",
      "           63,   63,   78,  135,  135,  158,  177,  177,  177,  225,  249,  249,\n",
      "          249,  326,  384,  445,  462,  462,  513,  513,  513,  513,  576,  640,\n",
      "          640,  154,  187,  154,  269,  153,   63,   63,   78,  106,  148,  148,\n",
      "          148,  148,  148,  148,  148,  644,  644,  662,  711,   70,   70,  103,\n",
      "          103,  160,  662,  662,  662,  662,  662,   63,   63,   77,   77,  413,\n",
      "          413,  428,  474,  522,  531,  531,  546,  546,  546,  613,  415,  415,\n",
      "          415,  479,  524,  524,  524,  408,  408,  408,   69,   69,   69,   69,\n",
      "           69,   69,   69,  428,  450,  450,  450,  481,  481,  581,  581,  581,\n",
      "          581,  581,  119,  143,  167,   56,   56,   71,   71,  119,  162,  162,\n",
      "          173,  210,  223,  276,  290,  326,  351,  351,  414,  414,  431,  431,\n",
      "          462,  493,  119,  163,  163,  173,  209,  252,  288,  322,  322,  339,\n",
      "          339,  393,  427,  427,  119,  119,   55,   55,   55,  117,  143,  182,\n",
      "          182,  213,  227,  227,  274,  304,  314,  314,  409,  452,  464,  464,\n",
      "          464,  534,  534,  549,  562,  569,  569, 1000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  33%|███▎      | 1/3 [00:00<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0, 1000,  661,  661,   53,   53,   67,  111,  198,  198,\n",
      "          198,  219,  219,  234,  234,  234,  234,  303,  303,  303,  324,  324,\n",
      "          339,  198,  198,  198,  220,  220,  231,  231,  231,   56,   56,   67,\n",
      "           67,  415,  415,  431,   71,  105,  105,  212,  212,  212,  328,  328,\n",
      "          328,  409,  409,  429,  429,  429,  429,  429,  519,  519,  519,  519,\n",
      "          585,  585,  585,   96,  449,  456,  456,  456,  456,  520,  520,  520,\n",
      "           57,   57,   70,  105,  144,  159,  408,  408,  430,  482,  525,  545,\n",
      "          572,  613,  629,  663,  694,   71,   71,  112,  204,  204,  204,  204,\n",
      "          204,  279,  409,   72,   72,  112,  112,  144,  144,  186,  186,  220,\n",
      "          220,  220,  220,  220,  296,  429,  429,  429,  429,  429,  545,  570,\n",
      "          604,  637,  525,  525,  525,  545,  545,  545,  559,  599,  599,  599,\n",
      "          614,  647,  647,  661,  568,  618,  562,  612,  504,  646,  684,  684,\n",
      "          705,  705,   64,   64,   78,   78,  122,  167,  196,  219,  415,  415,\n",
      "          429,  450,  476,  430,  491,   64,   64,   78,  106,  129,  178,  193,\n",
      "          250,  343,  345,  345,  404,  415,  415,  429,  450,  476,  429,  489,\n",
      "           63,   63,   77,   77,  103,  149,  178,  200,  529,  529,  544,  544,\n",
      "          544,  558,  558,  598,  598,  598,  613,  647,  647,  663,  559,  611,\n",
      "           70,  525,  409,  409,  428,  428,  428,  428,  428,  428,  428,  662,\n",
      "          687,  687,  687,  687,   63,   63,   78,  135,  135,  158,  177,  177,\n",
      "          177,  225,  249,  249,  249,  326,  384,  445,  462,  462,  513,  513,\n",
      "          513,  513,  576,  640,  640,  154,  187,  154,  269,  153,   63,   63,\n",
      "           78,  106,  148,  148,  148,  148,  148,  148,  148,  644,  644,  662,\n",
      "          711,   70,   70,  103,  103,  160,  662,  662,  662,  662,  662,   63,\n",
      "           63,   77,   77,  413,  413,  428,  474,  522,  531,  531,  546,  546,\n",
      "          546,  613,  415,  415,  415,  479,  524,  524,  524,  408,  408,  408,\n",
      "           69,   69,   69,   69,   69,   69,   69,  428,  450,  450,  450,  481,\n",
      "          481,  581,  581,  581,  581,  581,  119,  143,  167,   56,   56,   71,\n",
      "           71,  119,  162,  162,  173,  210,  223,  276,  290,  326,  351,  351,\n",
      "          414,  414,  431,  431,  462,  493,  119,  163,  163,  173,  209,  252,\n",
      "          288,  322,  322,  339,  339,  393,  427,  427,  119,  119,   55,   55,\n",
      "           55,  117,  143,  182,  182,  213,  227,  227,  274,  304,  314,  314,\n",
      "          409,  452,  464,  464,  464,  534,  534,  549,  562,  569,  569,  579,\n",
      "          605,  662,  675,  683,  718,  733,  733,   69,  135,  227,  227,   70,\n",
      "          177,  269,  336,  361,  403,  445,  445,  445,  445,  445,  519,  561,\n",
      "          561,  561,  561,  561,  561,   69,  135,  186,  228,  327,  412,  435,\n",
      "          461,  486,  528,  544,  544,   69,  136,  161,  235,  261,  337,  377,\n",
      "          402,  419,  419,  419,  483,  578,  452,  429,  458,  470,   64,   70,\n",
      "           96,  111,  178,  178,  178,  178,  228,  228,  228,  271,  271,  417,\n",
      "          417,  417,  431,  431,  431,  477,  477,  492,  512,  512,  567,  591,\n",
      "          615,  615,  434,  475,  475,  502,  502,   64,   69,  105,  169,  225,\n",
      "          263,  263,  274,  307,  326,  364,  364,  434,  485,  509,  509,  601,\n",
      "          601,  587,   54,   54,   69,  113,  170,  233,  249,   55,   55,   55,\n",
      "           55,  114,  164,  177,  177,  211,  211, 1000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     17\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (batch[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m example_indices \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(example_indices):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/T7/new btp/BTP/docvqa/modeling_layoutlm.py:278\u001b[0m, in \u001b[0;36mLayoutLMForTokenClassification.forward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    266\u001b[0m     input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# print(\"Input IDs\",input_ids.shape)\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# print(\"BBox\",bbox.shape)\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# print(\"LayoutLMModel Model Output:\",outputs[0].shape,)\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/T7/new btp/BTP/docvqa/modeling_layoutlm.py:210\u001b[0m, in \u001b[0;36mLayoutLMModel.forward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     head_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers\n\u001b[0;32m--> 210\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# print(\"embedding_output\",embedding_output.shape)\u001b[39;00m\n\u001b[1;32m    217\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    218\u001b[0m     embedding_output, extended_attention_mask, head_mask\u001b[38;5;241m=\u001b[39mhead_mask\n\u001b[1;32m    219\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/T7/new btp/BTP/docvqa/modeling_layoutlm.py:77\u001b[0m, in \u001b[0;36mLayoutLMEmbeddings.forward\u001b[0;34m(self, input_ids, bbox, token_type_ids, position_ids)\u001b[0m\n\u001b[1;32m     75\u001b[0m upper_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_position_embeddings(bbox[:, :, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     76\u001b[0m right_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_position_embeddings(bbox[:, :, \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m---> 77\u001b[0m lower_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_position_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m h_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_position_embeddings(\n\u001b[1;32m     79\u001b[0m     bbox[:, :, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m bbox[:, :, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m w_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_position_embeddings(\n\u001b[1;32m     82\u001b[0m     bbox[:, :, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m bbox[:, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     83\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "all_results = []\n",
    "table_data = []\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "        }\n",
    "        inputs[\"bbox\"] = batch[3]\n",
    "        inputs[\"token_type_ids\"] = (batch[2])\n",
    "        outputs = model(**inputs)\n",
    "    example_indices = batch[4]\n",
    "\n",
    "    for i, example_index in enumerate(example_indices):\n",
    "        eval_feature = features[example_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "        output = [to_list(output[i]) for output in outputs]\n",
    "\n",
    "        start_logits, end_logits = output\n",
    "        result = SquadResult(unique_id, start_logits, end_logits)\n",
    "        all_results.append(result)\n",
    "predictions_json = {}\n",
    "assert len(all_results)==len(features)\n",
    "for i in range(len(all_results)):\n",
    "    start_index = np.argmax(all_results[i].start_logits)\n",
    "    end_index = np.argmax(all_results[i].end_logits)\n",
    "    pred_answer = features[i].tokens[start_index:end_index+1]\n",
    "    pred_answer = ' '.join([x for x in pred_answer])\n",
    "    pred_text = pred_answer.replace(' ##', '')\n",
    "    question = features[i].tokens[1:features[i].tokens.index('[SEP]')]\n",
    "    question_text = ' '.join([x for x in question])\n",
    "    question_text = question_text.replace(' ##', '')\n",
    "    table_data.append([question_text, pred_text])\n",
    "    # print(question_text)\n",
    "    # print(pred_text) \n",
    "\n",
    "\n",
    "headers = [\"Question\", \"Answer\"]\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
